{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: ['unet_xception_64_augment:8']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmpevanfccl/57da9d98.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 10:54:25+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 10:54:25+01:00 Registering the environment.\n",
      "2022-03-12 10:54:27+01:00 Use the existing image.\n",
      "2022-03-12 10:54:30+01:00 Submitting deployment to compute.\n",
      "2022-03-12 10:54:34+01:00 Checking the status of deployment unet-xception-64-augment-839a..\n",
      "2022-03-12 10:56:47+01:00 Checking the status of inference endpoint unet-xception-64-augment-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['deeplab_v3plus_80_augment:1']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmpp8eow58n/cfe65d0c.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:03:08+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:03:08+01:00 Registering the environment.\n",
      "2022-03-12 11:03:10+01:00 Use the existing image.\n",
      "2022-03-12 11:03:12+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:03:16+01:00 Checking the status of deployment deeplab-v3plus-80-augment-839a..\n",
      "2022-03-12 11:05:25+01:00 Checking the status of inference endpoint deeplab-v3plus-80-augment-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['fcn_8_128:2']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmpy_zxttvq/43cc3d67.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:05:53+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:05:53+01:00 Registering the environment.\n",
      "2022-03-12 11:05:54+01:00 Use the existing image.\n",
      "2022-03-12 11:05:54+01:00 Generating deployment configuration.\n",
      "2022-03-12 11:05:56+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:06:01+01:00 Checking the status of deployment fcn-8-128-839a..\n",
      "2022-03-12 11:07:33+01:00 Checking the status of deployment fcn-8-128-839a..\n",
      "2022-03-12 11:14:56+01:00 Checking the status of inference endpoint fcn-8-128-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['unet_xception_128:6']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmp5mq_wa_6/6f4f52fc.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:15:26+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:15:26+01:00 Registering the environment.\n",
      "2022-03-12 11:15:29+01:00 Use the existing image.\n",
      "2022-03-12 11:15:29+01:00 Generating deployment configuration.\n",
      "2022-03-12 11:15:31+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:15:36+01:00 Checking the status of deployment unet-xception-128-839a..\n",
      "2022-03-12 11:20:20+01:00 Checking the status of inference endpoint unet-xception-128-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['deeplab_v3plus_128:2']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmph_hj8exo/d2e5ceeb.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:25:13+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:25:14+01:00 Registering the environment.\n",
      "2022-03-12 11:25:16+01:00 Use the existing image.\n",
      "2022-03-12 11:25:16+01:00 Generating deployment configuration.\n",
      "2022-03-12 11:25:18+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:25:23+01:00 Checking the status of deployment deeplab-v3plus-128-839a..\n",
      "2022-03-12 11:36:46+01:00 Checking the status of inference endpoint deeplab-v3plus-128-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['deeplab_v3plus_160:1']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmpvzvtbf0i/4cca09cf.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:37:14+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:37:14+01:00 Registering the environment.\n",
      "2022-03-12 11:37:16+01:00 Use the existing image.\n",
      "2022-03-12 11:37:18+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:37:22+01:00 Checking the status of deployment deeplab-v3plus-160-839a..\n",
      "2022-03-12 11:39:23+01:00 Checking the status of inference endpoint deeplab-v3plus-160-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['fcn_8_256:2']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmp1obkscwi/0b5c1513.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:43:06+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:43:06+01:00 Registering the environment.\n",
      "2022-03-12 11:43:07+01:00 Use the existing image.\n",
      "2022-03-12 11:43:09+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:43:13+01:00 Checking the status of deployment fcn-8-256-839a..\n",
      "2022-03-12 11:52:21+01:00 Checking the status of inference endpoint fcn-8-256-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['unet_xception_256_augment:4']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmp7dtsiuup/2a1b51cc.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:52:58+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:52:59+01:00 Registering the environment.\n",
      "2022-03-12 11:53:00+01:00 Use the existing image.\n",
      "2022-03-12 11:53:00+01:00 Generating deployment configuration.\n",
      "2022-03-12 11:53:02+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:53:06+01:00 Checking the status of deployment unet-xception-256-augment-839a..\n",
      "2022-03-12 11:56:24+01:00 Checking the status of inference endpoint unet-xception-256-augment-839a.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Models: ['unet_xception_256:5']\n",
      "Entry script: azureml/predict.py\n",
      "Environment variables: {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE', 'AZUREML_SOURCE_DIRECTORY': 'azureml', 'AZUREML_ENTRY_SCRIPT': 'azureml/predict.py'}\n",
      "Environment dependencies: ['python=3.8', {'pip': ['tensorflow==2.7.0', 'azureml-defaults', 'azureml-sdk', 'inference-schema', 'numpy', 'matplotlib', 'Pillow']}]\n",
      "Environment docker image: mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20220208.v1\n",
      "CPU requirement: 3.8, Memory requirement: 15GB\n",
      "Uploading dependency /tmp/tmpboow73q7/54b753aa.tar.gz.\n",
      "Request submitted, please run wait_for_deployment(show_output=True) to get deployment status.\n",
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2022-03-12 11:56:57+01:00 Creating Container Registry if not exists.\n",
      "2022-03-12 11:56:57+01:00 Registering the environment.\n",
      "2022-03-12 11:56:58+01:00 Use the existing image.\n",
      "2022-03-12 11:56:58+01:00 Generating deployment configuration.\n",
      "2022-03-12 11:57:00+01:00 Submitting deployment to compute.\n",
      "2022-03-12 11:57:04+01:00 Checking the status of deployment unet-xception-256-839a."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azureml.core import Environment, Model, Workspace\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "load_dotenv()\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "AZURE_RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "AZURE_WORKSPACE_NAME = os.getenv(\"AZURE_WORKSPACE_NAME\")\n",
    "\n",
    "\n",
    "version = str(uuid.uuid4())[:4]\n",
    "\n",
    "source_directory = \"./azureml\"\n",
    "experiment_name = \"oc-p8-experiment-1\"\n",
    "\n",
    "model_names = [\n",
    "    \"unet_xception_64_augment\",\n",
    "    \"deeplab_v3plus_80_augment\",\n",
    "    \"fcn_8_128\",\n",
    "    \"unet_xception_128\",\n",
    "    \"deeplab_v3plus_128\",\n",
    "    \"deeplab_v3plus_160\",\n",
    "    \"fcn_8_256\",\n",
    "    \"unet_xception_256_augment\",\n",
    "    \"unet_xception_256\",\n",
    "    \"deeplab_v3plus_256\",\n",
    "    \"deeplab_v3plus_320_augment\",\n",
    "    \"fcn_8_512\",\n",
    "    \"unet_xception_512\",\n",
    "    \"deeplab_v3plus_512\",\n",
    "    \"deeplab_v3plus_640\",\n",
    "]\n",
    "\n",
    "\n",
    "# connect to your workspace\n",
    "ws = Workspace(\n",
    "    subscription_id=AZURE_SUBSCRIPTION_ID,\n",
    "    resource_group=AZURE_RESOURCE_GROUP,\n",
    "    workspace_name=AZURE_WORKSPACE_NAME,\n",
    ")\n",
    "\n",
    "\n",
    "env_name = experiment_name + \"-predict\"\n",
    "try:\n",
    "    env = Environment.get(workspace=ws, name=env_name)\n",
    "except:\n",
    "    env = Environment.from_conda_specification(\n",
    "        name=env_name,\n",
    "        file_path=Path(source_directory, \"conda_dependencies.yml\"),\n",
    "    )\n",
    "    env.inferencing_stack_version = \"latest\"\n",
    "    env.register(workspace=ws)\n",
    "\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script=\"predict.py\",\n",
    "    environment=env,\n",
    ")\n",
    "\n",
    "\n",
    "for model_name in model_names:\n",
    "    aci_config = AciWebservice.deploy_configuration(\n",
    "        cpu_cores=3.8,\n",
    "        memory_gb=15,\n",
    "        auth_enabled=True,\n",
    "    )\n",
    "\n",
    "    model = Model(ws, model_name)\n",
    "\n",
    "    service = Model.deploy(\n",
    "        workspace=ws,\n",
    "        name=model_name.replace(\"_\", \"-\") + \"-\" + version,\n",
    "        models=[model],\n",
    "        inference_config=inference_config,\n",
    "        deployment_config=aci_config,\n",
    "        overwrite=True,\n",
    "        show_output=True,\n",
    "    )\n",
    "\n",
    "    service.wait_for_deployment(show_output=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8c2c830ee423e437a1692c62d7ec975894aa2a778a66c89e5cf73f15552e52b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
